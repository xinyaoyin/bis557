---
title: "homework-5"
author: "Xinyao Yin"
date: "12/19/2018"
output: html_document
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The Homework 5 vignette}
-->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1 
```{r}
# load libraries
library(keras)
library(glmnet)
library(doMC)
registerDoMC()
```

According to notes from class:
```{r}
# install_keras()
# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)

# original model 
s <- sample(seq_along(y_train), 1000) 
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")


# Estimate out-of-sample prediction accuracy using minimum lambda (as done in class)
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t <- table(as.vector(preds), y_test)
accuracy1 = sum(diag(t)) / sum(t) 
accuracy1
# out-of-sample accuracy = 0.8475
```

Using LASSO we get the out-of-sample accuracy to be 0.8475.

 
Then, we try to increase out-of-sample prediction accuracy by extracting predictive features. We can decrease lambda to include more predictive features. We use the minimum lambda minus 1 standard error as the penalty.

```{r}
# Estimate out-of-sample prediction accuracy using minimum lambda - 1se
se <- fit$lambda.1se-fit$lambda.min
lamda.minus.1se <- fit$lambda.min-se
preds2 <- predict(fit$glmnet.fit, x_test, s = lamda.minus.1se, 
                 type = "class")
t2 <- table(as.vector(preds2), y_test)
accuracy2 = sum(diag(t2)) / sum(t2) 
accuracy2
# out-of-sample accuracy = 0.8592
```

From the above results we can see that we have increased the out-of-sample prediction accuracy from 0.8475 to 0.8592. Including more predictive features have increased the out-of-sample prediction accuracy. 


# Question 2
I download the data from https://www.kaggle.com/crawford/emnist#emnist-letters-train.csv 
```{r}
# load data
load('/Users/graceyin/Desktop/bis557/data/emnist.rda')

# split the data into train (90%) and valid (10%) set.
set.seed(1212)

# 90% of the sample size
smp_size <- floor(0.9 * nrow(emnist))
train_ind <- sample(seq_len(nrow(emnist)), size = smp_size)

# training and testing datasets 
train <- emnist[train_ind, ]
test <- emnist[-train_ind, ]

# process data
x_train <-as.matrix(train[,-1])
x_test <-as.matrix(test[,-1])
y_train<-train[,1]
y_test<-test[,1]
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

# convert RGB values into [0,1] range
x_train <- x_train/255
x_test <- x_test/255

cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')

# convert class vectors to binary class matrices
y_train <- to_categorical(as.matrix(y_train-1), num_classes = 26)
y_test <- to_categorical(as.matrix(y_test-1), num_classes = 26)


# Fit the model 
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                input_shape = c(28, 28, 1),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")


# Compile model and fit a convolutional neural network 
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))

history <- model %>%
fit(x_train, y_train, epochs = 10,
    validation_data = list(x_test, y_test))
print(history)
```


Next, we try to improve the model by tuning the parameters. After trying a few combinations, I found that using a (3,3) kernel and a dropout rate of 0.4 gives a good improvement. 

```{r}
model_2 <- keras_model_sequential()
model_2 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
input_shape = c(28, 28, 1),
                padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
              padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.2) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
              padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.4) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")

# Compile model and fit a convolutional neural network 
model_2 %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))

history_2 <- model_2 %>%
fit(x_train, y_train, epochs = 6,
    validation_data = list(x_test, y_test))
print(history_2)
```

As we can see that the model accuracy has improved from 88.26% in the original model to 90.95% in the new model. 


# Question 3
We know the following functions from CASL textbook:
```{r}
# casl_nn_init_weights API
# Initiate weight matrics for a dense neural network.
#
# Args:
#     layer_sizes: A vector(numeric) showing the size of each layer
#
# Returns:
#     A list containing initialized weights and biases.
casl_nn_init_weights <- function(layer_sizes){
  len<- length(layer_sizes) - 1
  weights <- vector("list", len)
  
  for (i in seq_len(len)){
    w <- matrix(rnorm(layer_sizes[i] * layer_sizes[i + 1]),
            ncol = layer_sizes[i],
            nrow = layer_sizes[i + 1])
    weights[[i]] <- list(w=w,
                     b=rnorm(layer_sizes[i + 1]))
  }
  weights
}
weight <- casl_nn_init_weights(c(1, 25, 1))
```

```{r}
# casl_util_ReLU API
# Apply a rectified linear unit (ReLU) to a vector / matrix.
#
# Args:
#     vec: A vector(numeric) or matrix(numeric).
#
# Returns:
#     The original input with negative values truncated to 0.
casl_util_ReLU <- function(vec){
  vec[vec < 0] <- 0
  vec 
}
```

```{r}
# casl_util_ReLU_derivative API
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     vec: A vector(numeric) or matrix(numeric).
#
# Returns:
#     Returned with positive values to 1 and negative values to 0.
casl_util_ReLU_derivative <- function(vec){
  res <- vec * 0
  res[vec > 0] <- 1
  res
}
```

```{r}
# casl_util_mad_p API
# Derivative of the mean absolute deviance (MAD).
#
# Args:
#     y: A vector(numeric) of responses.
#     pred: A vector(numeric) of predicted responses.
#
# Returns:
#     Returned current derivative MAD
casl_util_mad_p <- function(y, pred){
  dev <- c()
  for(i in seq_along(pred)){
    if(pred[i] >= y[i]) dev[i] <- 1
    else dev[i] <- -1
  }
  dev
}
```

```{r}
# casl_nn_forward_prop API
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     input: A vector(numeric) representing one row of the input.
#     weights: A list of weights built by casl_nn_init_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_forward_prop <- function(input, weights, sigma){
  len <- length(weights)
  z <- vector("list", len)
  a <- vector("list", len)
  
  for (i in seq_len(len)){
    a_i1 <- if(i == 1) input else a[[i - 1]]
    z[[i]] <- weights[[i]]$w %*% a_i1 + weights[[i]]$b
    a[[i]] <- if (i != len) sigma(z[[i]]) else z[[i]]
  }
  list(z = z, a = a)
}
```

```{r}
# casl_nn_backward_prop API
# Apply backward propagation algorithm.
#
# Args:
#     input: A vector(numeric) representing one row of the input.
#     output: A vector(numeric) representing one row of the response.
#     weights: A list created by casl_nn_init_weights.
#     forwardpp_obj: Output of the function casl_nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_backward_prop <- function(input, output, weights, forwardpp_obj, sigma_p, f_p){
  z <- forwardpp_obj$z
  a <- forwardpp_obj$a
  len <- length(weights)
  grad_z <- vector("list", len)
  grad_w <- vector("list", len)
  
  for (i in rev(seq_len(len))){
    if (i == len) {
      grad_z[[i]] <- f_p(output, a[[i]])
      }
    else {
      grad_z[[i]] <- (t(weights[[i + 1]]$w) %*% grad_z[[i + 1]]) * sigma_p(z[[i]])
      }
    a_j1 <- if(i == 1) input else a[[i - 1]]
    grad_w[[i]] <- grad_z[[i]] %*% t(a_j1)
    }
  list(grad_z = grad_z, grad_w = grad_w)
}
```

```{r}
# casl_nn_sgd_mad API
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A data(numeric) matrix.
#     y: A vector(numeric) of responses.
#     layers_sizes: A vector(numeric) showing the sizes of layers in NN.
#     epochs: Integer number of epochs to computer.
#     lr: Learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd_mad <- function(X, y, layer_sizes, epochs, lr, weights=NULL){
  if (is.null(weights)){
    weights <- casl_nn_init_weights(layer_sizes)
  }
  
  # for every individual, update the weights and repeat the procedure over all individuals.
  for (epoch in seq_len(epochs)){
    # propergations
    for (i in seq_len(nrow(X))){ 
      # excute forward propergation
      forwardpp_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      # excute backward propergation
      backwardpp_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
                                     forwardpp_obj, casl_util_ReLU_derivative, casl_util_mad_p)
      # update weights matrics
      for (j in seq_along(weights)){
        weights[[j]]$b <- weights[[j]]$b -lr * backwardpp_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w -lr * backwardpp_obj$grad_w[[j]]
      }
    }
  }
  weights
}
```

```{r}
# casl_nn_predict function
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
casl_nn_predict <- function(weights, X_test){
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  
  # excute prediction by forward propergation
  for (i in seq_len(nrow(X_test))){
    a <- casl_nn_forward_prop(X_test[i,], weights, casl_util_ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
    }
  y_hat
}
```

Now, we perform data simulation to test the functions using Mean Absolute Deviation
```{r}
library(tidyverse)
# simulate data using MAD
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
out <- X[, 1, drop = FALSE]^2 + rnorm(1000, sd = 0.1)

# create some outliers
ind <- sample(seq_along(out), 100)
out[sort(ind)] <- c(runif(50, -10, -5), runif(50, 5, 10))

# create weights and do prediction
weights <- casl_nn_sgd_mad(X, out, layer_sizes = c(1, 25, 1), epochs = 15, lr = 0.01)

y_prediction <- casl_nn_predict(weights, X)

# visualiza the true value and predicted value
d <- tibble(x = as.vector(X), y_prediction = as.vector(y_prediction),
            y = X[, 1]^2, out = as.vector(out))

# create a plot of mean absolute deviation
ggplot(d) + 
  geom_point(aes(x = x, y = out)) +
  geom_point(aes(x = x, y = y_prediction, color = "red", alpha = 1.5)) +
  labs(x = "x", y = "y values", title = "True and Predicted Values Using Mean Absolute Deviance") +
  theme(legend.position="None") + labs(subtitle="Red: Predicted; Black: True") +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

From the plot we can see that the predicted value are almost the same as the true values (align pretty well). This shows that even with outliers, the predicted values are robust and are not influenced by outliers. The neural network and stochastic gradient descent algorithm have robustness to outliers when using mean assolute deviance (MAD).


Reference from CASL Textbook:
casl_util_mse_derivative API
```{r, echo=FALSE}
# Derivative of the mean squared error (MSE) function.
#
# Args:
#     y: A numeric vector of responses.
#     pred: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MSE function.
casl_util_mse_derivative <- function(y, pred){
  2 * (pred - y)
}
```

casl_nn_sgd_mse API
```{r, echo=FALSE}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in
#            the neural network.
#     epochs: Integer number of epochs to computer.
#     lr: Learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd_mse <- function(X, y, layer_sizes, epochs, lr, weights=NULL){
  if (is.null(weights)){
    weights <- casl_nn_init_weights(layer_sizes)
  }
  ## for every individual, update the weights; repeat the procedure over all individuals.
  for (epoch in seq_len(epochs)){
    # propergations
    for (i in seq_len(nrow(X))){ 
      # forward propergations
      forwardpp_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      # backward propergations
      backwardpp_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
                                     forwardpp_obj, casl_util_ReLU_derivative, casl_util_mse_derivative)
      # update weights
      for (j in seq_along(weights)){
        weights[[j]]$b <- weights[[j]]$b -lr * backwardpp_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w -lr * backwardpp_obj$grad_w[[j]]
      }
    }
  }
  weights
}
```

Now we perform data simulation to test the functions using Mean Squared Error
```{r, echo=FALSE}
# simulate data using MSE
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
out <- X[,1,drop = FALSE]^ 2 + rnorm(1000, sd = 0.1)

# create some outliers
ind <- sample(seq_along(out), 100)
out[sort(ind)] <- c(runif(50, -10, -5), runif(50, 5, 10))

# create weights and do prediction
weights <- casl_nn_sgd_mse(X, out, layer_sizes = c(1, 25, 1), epochs = 15, lr = 0.01)

y_prediction <- casl_nn_predict(weights, X)


# visualiza the true value and predicted value
d <- tibble(x = as.vector(X), y_prediction = as.vector(y_prediction),
            y = X[,1]^2, out = as.vector(out))

# plot results with mean square error
ggplot(d) + 
  geom_point(aes(x = x, y = out)) +
  geom_point(aes(x = x, y = y_prediction, color = "red", alpha = 1.5)) +
  labs(x = "x", y = "y values", title = "True and Predicted Values Using Mean Squared Error") +
  theme(legend.position="None") + labs(subtitle="Red: Predicted; Black: True") +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

From the plot we can see that if we use MSE instead of MAD, the SGD method is not as robust as before. But it still gives a very good prediction. As shown in the above plot, the predicted values are a little apart from the true values, indicating the predicted values might be influenced by outliers. But in general the prediction is still very close. 